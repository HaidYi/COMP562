{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latex definitions in here. Do not remove this cell.**\n",
    "$$\n",
    "\\renewcommand{\\vvec}[2]{\\left[ \\begin{array}{c} \\mathbf{#1}\\\\ \\mathbf{#2} \\end{array}\\right]}\n",
    "\\renewcommand{\\mmat}[4]{\\left[ \\begin{array}{cc} \\mathbf{#1}&\\mathbf{#2}\\\\ \\mathbf{#3}&\\mathbf{#4} \\end{array}\\right]}\n",
    "\\renewcommand{\\aaa}{\\mathbf{a}}\n",
    "\\renewcommand{\\AAA}{\\mathbf{A}}\n",
    "\\renewcommand{\\xyvec}{\\left[ \\begin{array}{c} \\xx\\\\\\yy \\end{array} \\right]}\n",
    "\\renewcommand{\\xyvecc}{\\left[ \\begin{array}{c} x^1\\\\y^1 \\end{array} \\right]}\n",
    "\\renewcommand{\\mm}{\\mathbf{m}}\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\ee}{\\mathbf{e}}\n",
    "\\renewcommand{\\ww}{\\mathbf{w}}\n",
    "\\renewcommand{\\XX}{\\mathbf{X}}\n",
    "\\renewcommand{\\YY}{\\mathbf{Y}}\n",
    "\\renewcommand{\\WW}{\\mathbf{W}}\n",
    "\\renewcommand{\\VV}{\\mathbf{V}}\n",
    "\\renewcommand{\\DD}{\\mathbf{D}}\n",
    "\\renewcommand{\\dd}{\\mathbf{d}}\n",
    "\\renewcommand{\\ZZ}{\\mathbf{Z}}\n",
    "\\renewcommand{\\CC}{\\mathbf{C}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\ddelta}{\\boldsymbol{\\mathbf{\\delta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ \\big| }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\EE}{\\mathbb{E}}\n",
    "\\renewcommand{\\EEE}{\\mathbf{E}}\n",
    "\\renewcommand{\\KL}{\\textrm{KL}}\n",
    "\\renewcommand{\\Bound}{\\mathcal{B}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\corr}[2]{\\textrm{corr}(#1,#2)}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmax}{\\mathop{\\textrm{argmax}}}\n",
    "\\renewcommand{\\new}{\\textrm{new}}\n",
    "\\renewcommand{\\old}{\\textrm{old}}\n",
    "\\renewcommand{\\bb}{\\mathbf{b}}\n",
    "\\renewcommand{\\ba}{\\mathbf{a}}\n",
    "\\renewcommand{\\bg}{\\mathbf{g}}\n",
    "\\renewcommand{\\BB}{\\mathbf{B}}\n",
    "\\renewcommand{\\BA}{\\mathbf{A}}\n",
    "\\renewcommand{\\BC}{\\mathbf{C}}\n",
    "\\renewcommand{\\UU}{\\mathbf{U}}\n",
    "\\renewcommand{\\uu}{\\mathbf{u}}\n",
    "\\renewcommand{\\hh}{\\mathbf{h}}\n",
    "\\renewcommand{\\SSS}{\\mathbf{S}}\n",
    "\\renewcommand{\\sss}{\\mathbf{s}}\n",
    "\\renewcommand{\\rr}{\\mathbf{r}}\n",
    "\\renewcommand{\\tr}[1]{\\textrm{tr}\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmin}{\\mathop{\\textrm{argmin}}}\n",
    "\\renewcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\renewcommand{\\sign}[1]{\\textrm{sign}\\left(#1\\right)}\n",
    "\\renewcommand{\\minimize}{\\mathop{\\textrm{minimize}}}\n",
    "\\renewcommand{\\subjectto}{\\mathop{\\textrm{subject to}}}\n",
    "\\renewcommand{\\relu}{\\textrm{ReLU}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Submission instructions\n",
    "\n",
    "* Deadline: 12/5/2016 23:59.\n",
    "* Submit your homework (.ipynb file) to **jsilva@cs.unc.edu** and **poirson@cs.unc.edu** using submission e-mail template:\n",
    "\n",
    "```\n",
    "To: jsilva@cs.unc.edu\n",
    "Cc: poirson@cs.unc.edu\n",
    "From: Super Student\n",
    "Subject: HW3 submission\n",
    "\n",
    "First Name: Super\n",
    "Last Name: Student\n",
    "PID: 11111111\n",
    "\n",
    "Colaborated with: \n",
    "First Name: Another\n",
    "Last Name: Student\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing Filter Response\n",
    "Given a filter $\\ww$, derive closed-form solution for a vector $\\xx$ that maximizes inner product between $\\ww^T\\xx$ under assumption that $\\sum_i x_i^2 = 1$.\n",
    "\n",
    "To accomplish this first, we write out the optimization problem\n",
    "\\begin{eqnarray}\n",
    "\\mathop{\\textrm{maximize}}_{\\xx} && \\ww^T\\xx \\\\\n",
    "\\textrm{subject to} && \\sum_i x_i^2 = 1\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **(1pt)** Write out a Lagrangian. Hint: Lagrangian combines the objective and constraint. Contraint $\\sum_i x_i^2 = 1$ can be rewritten as $1 - \\sum_i x_i^2 = 0$. You can use $\\lambda$ to denote variable associated with the constraint.\n",
    "\n",
    "$$\n",
    "L(\\xx,\\lambda) = ... + \\lambda(...)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **(1pt)** Take partial derivatives with respect to $x_i$ and $\\lambda$. Hint: Use the fact that $ \\ww^T\\xx = \\sum_i w_ix_i.$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial L(\\xx,\\lambda)}{\\partial x_i} &=& ... \\\\\n",
    "\\frac{\\partial L(\\xx,\\lambda)}{\\partial \\lambda} &=& ...\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **(1pt)** Equate the partial derivatives to zero, express $x_i$ in terms of $w_i$ and $\\lambda$, and substitute $x_i$ in the partial derivative $\\frac{\\partial L(\\xx,\\lambda)}{\\partial \\lambda}$. Solve for $\\lambda$.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "...&=& 0 \\\\\n",
    "... &=& ...\\\\\n",
    "... &=&  ... \\\\\n",
    "... &=& \\lambda \\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **(1pt)** Express optimal $x_i$ in terms of $\\ww$.\n",
    "$$\n",
    "x_i = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **(1pt)** Implement filter_maximizer which takes as input a filter and returns a vector of norm 1, which has largest inner product with the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "def filter_maximizer(w):\n",
    "    return ...\n",
    "    \n",
    "w = np.asarray([-1.0,1.0,-1.0,1.0])\n",
    "x = filter_maximizer(w)\n",
    "assert(np.isclose(np.linalg.norm(x),1.0))\n",
    "print(x)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Understanding ReLU networks\n",
    "\n",
    "We will play with Rectified Linear Units (ReLU). We will show that output of a deep network composed of ReLU layers is linear near almost all feature vectors. Then we will demonstrate that globally these same networks can capture a non-linear function.\n",
    "\n",
    "Rectified Linar Unit activation\n",
    "$$\n",
    "\\relu(z) = \\max(z,0) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1pt)** Decompose ReLU activation into a product of two separate functions by filling in the details\n",
    "\n",
    "Let ReLU activation be expressed in terms of two separate functions\n",
    "$$\n",
    "\\relu(z) = r(z)f(z)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "r(z) = \\begin{cases}\n",
    "1, & ... \\\\\n",
    "0, & ...\n",
    "\\end{cases}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "f(z) = z\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **(1pt)** Complete forward propagation definition, assuming that $l$th layer weights are $\\WW^l$ and $\\bb^l$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\hh^0 &=& \\xx \\\\\n",
    "\\zz^l &=& ... \\\\\n",
    "h^l_i &=& \\relu(z^l_i) \\\\\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **(1pt)** Introduce variables $m^l_i$ one per each unit in layer and rewrite forward propagation. Note: $zz^l$ stays same as above.\n",
    "\\begin{eqnarray}\n",
    "\\hh^0 &=& \\xx \\\\\n",
    "\\zz^l &=& ... \\\\\n",
    "m^l_i &=& r(...)\\\\\n",
    "h^l_i &=& m^l_i z^l_i \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can thinkg of $\\mm^l$ as a mask. It masks out information from $\\zz^l$. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal is to show that for input $\\xx+\\dd$ change in $\\hh$ will be equal to $\\WW\\dd$ as long as $\\dd$ is small enough.\n",
    "Further we will characterize just how large can the $\\dd$ be and still have that change be equal to $\\WW\\dd$.**\n",
    "\n",
    "We will use the fact that there exists $c$ such that\n",
    "$$\n",
    "\\max_i \\abs{\\ww_i^T\\dd} \\leq  c \\max_j \\abs{d_j}\n",
    "$$ \n",
    "For the curious: to show this, use submultiplicativity of max-norm and let $c$ be largest singular value of $\\WW$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now going to consider a single ReLU layer taking input $\\xx$ and outputing $\\hh$. \n",
    "We will denote weights by $\\WW$, a single row of $\\WW$ as $\\ww_i$, bias terms by $\\bb$, input into activation functions $\\zz$, and masks by $\\mm$.  We will assume that $\\abs{z_i} > \\epsilon$ for a specific $\\xx$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3) **(1pt)** Write equation for $z_i$. You can use $\\ww_i$ to denote $i$th row of matrix $\\WW$.\n",
    "$$\n",
    "z_i = ... + ...\n",
    "$$\n",
    "\n",
    "Now we will express the change in $z_i$ changes when $\\xx$ changes.\n",
    "Let $\\tilde{z}_i$ denote activation input for $\\tilde{\\xx} = \\xx-\\dd$:\n",
    "$$\n",
    "\\tilde{z}_i = ... + ...\n",
    "$$\n",
    "First replace $\\tilde{\\xx}$ with $\\xx - \\dd$\n",
    "$$\n",
    "\\tilde{z}_i = ... + ....\n",
    "$$\n",
    "Now, express $\\tilde{z}_i$ in terms of $z_i$, $\\ww_i$, and $\\dd$\n",
    "$$\n",
    " \\tilde{z}_i = ... - ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **(1pt)** If $z_i \\geq \\epsilon$, use $\\ww_i^T\\dd \\leq c \\max_j \\abs{d_j}$ to obtain an upper bound on $\\max_j \\abs{d_j}$ that ensures that $\\tilde{z_i}$ is larger than $\\frac{\\epsilon}{2}$.\n",
    "$$\n",
    "\\max_j \\abs{d_j} \\leq ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **(1pt)** If $-z_i \\geq \\epsilon$, use $\\ww_i^T\\dd \\leq c \\max_j \\abs{d_j}$ to obtain an upper bound on $\\max_j \\abs{d_j}$ that ensures that $-\\tilde{z_i}$ is smaller than -$\\frac{\\epsilon}{2}$.\n",
    "$$\n",
    "\\max_j \\abs{d_j} \\leq ...\n",
    "$$\n",
    "Hint: it is the same bound as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "6) **(1pt)** If absolute value of each activation input is above $\\epsilon$, perturbation of the input by a vector $\\dd$, whose entries lie in interval $[-...,...]$ ensures that the mask stays the same, $m(\\zz) = m(\\tilde{\\zz})$.\n",
    "\n",
    "\n",
    "If preturbation vector is bounded as above, the output of the ReLU layer for $\\xx$ is \n",
    "$$ h_i = m(z_i)z_i $$ \n",
    "and for $\\tilde{\\xx}=\\xx-\\dd$, output is equal to\n",
    "$$ h_i = m(\\tilde{z}_i)\\tilde{z}_i = m(z_i)\\tilde{z}_i$$\n",
    "\n",
    "Express output of ReLU unit, $h_i$, in terms of $m(z_i),\\xx,\\dd,\\ww_i,b_i$ for input $\\xx - \\dd$\n",
    "$$\n",
    "h_i = ...\n",
    "$$\n",
    "\n",
    "Hence as long as the perturbation by $\\dd$ does not change the mask, the Rectified Linear Unit is equivalent to a linear unit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **(1pt)** Assume that a two layer ReLU net is given. Each of its activation inputs has magnitude greater than $\\epsilon$.\n",
    "Assume that $\\ww_i^T\\dd \\leq c \\max_j \\abs{d_j}$ for weights in both layers.\n",
    "What is the magnitude of entries of perturbation $\\dd$ that maintains the masks in both layers?\n",
    "$$\n",
    "\\max_j \\abs{d_j} \\leq ....\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
